## 基础知识和相关专业术语

### 线性可分割

存在一条直线分割两类

### 超参数

人为规定好的参数 C

算法的超参数越多，需要调整的得也就越多，算法的自动性就越低。

### 支持向量机 是超参数恨少的算法模型

### 比较多的超参数算法

- 人工神经网络
- 卷积神经网络

### 线性不可分

### 原问题 prime problem 的定义

### 对偶问题 dual problem

### 对偶差距

## 核函数戏法

### 交叉验证

### 过拟合

### 5 折交叉验证

5000 个训练数据，分成 5 个组，每组测试数据是 1000 个。组分别为 ABCDE。
ABCD 训练，E 测试。
... ...
BCDE 训练，A 测试。
最后平均

### 留一法

就是训练数据比较小的时候，留一个数据来做测试。
level-one-out

## 三个神经网络的建议

- 在训练集**目标函数的平均值(cost)**会随着训练的深入而不断减小，如果这个指标增大，停下来

  - 采用的模型不够复杂
  - 已经训练好了

- 分出一些验证集(validation set), 训练的本质目标是在验证集上获取最大识别率

  - 训练一段时间后，必须在验证集上测试识别率

- 注意调整学习率
  - 如果刚训练几步损失函数 cost 就增加了，一般来说是学习率太高了
  - 如果每次 cost 变化很小，说明学习率太低

## 训练神经网络的各种经验

- 目标函数可以加入**正则项(regularization term)**

- 加了正则项的目标函数前向计算

- 训练数据归一化
